{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN22-Gen: Evaluation Notebook (eng → 16 Indic languages)\n",
        "\n",
        "This notebook loads two JSONL files:\n",
        "1. `predictions.jsonl` — each line: `{\"id\": ..., \"english_og\": ..., \"translations\": {\"hi\": \"...\", \"bn\": \"...\", ...}}`\n",
        "2. `ground_truths.jsonl` — ai4bharat/IN22-Gen style rows (one JSON object per line) with columns such as `hin_Deva`, `ben_Beng`, etc. and an `id` field.\n",
        "\n",
        "It will merge both files on `id` and compute the following metrics per language:\n",
        "- chrF (character n-gram F-score)\n",
        "- chrF++ (character+word n-gram variant)\n",
        "- sacreBLEU (corpus BLEU via sacrebleu)\n",
        "- COMET (optional; requires `comet` package and model checkpoint)\n",
        "\n",
        "**Output:** a CSV and a printed DataFrame with per-language metrics and counts.\n",
        "\n",
        "----\n",
        "\n",
        "Notes:\n",
        "- The notebook tries to compute COMET if the `comet` package is available and a model checkpoint is loadable. If not, COMET is skipped and other metrics are computed.\n",
        "- You can change input filenames in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f82d54",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "!pip install sacrebleu pandas tqdm numpy\n",
        "!pip install git+https://github.com/Unbabel/comet.git\n",
        "\n",
        "print('If you need to install dependencies, uncomment the pip install lines in this cell and run it.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d302752",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "from sacrebleu.metrics import CHRF\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "GROUND_TRUTHS_PATH = '/nfs/bds/translation_benchmark/datasets/IN22-Gen/data/train-00000-of-00001_with_ids.jsonl' \n",
        "PREDICTIONS_PATH = '/nfs/home/bhargav.patel/bhargav/MT/ds_IN22_enhanced_trans.jsonl'  \n",
        "OUTPUT_CSV = '/nfs/bds/translation_benchmark/INT2/in22_eval_results.csv'\n",
        "\n",
        "LANG_CODE_MAP = {\n",
        "    'as': 'asm_Beng',\n",
        "    'bn': 'ben_Beng',\n",
        "    'gu': 'guj_Gujr',\n",
        "    'hi': 'hin_Deva',\n",
        "    'kn': 'kan_Knda',\n",
        "    'mai': 'mai_Deva',\n",
        "    'ml': 'mal_Mlym',\n",
        "    'mr': 'mar_Deva',\n",
        "    'ne': 'npi_Deva',\n",
        "    'or': 'ory_Orya',\n",
        "    'pa': 'pan_Guru',\n",
        "    'sa': 'san_Deva',\n",
        "    'sdd': 'snd_Deva',  \n",
        "    'ta': 'tam_Taml',\n",
        "    'te': 'tel_Telu',\n",
        "    'ur': 'urd_Arab'\n",
        "}\n",
        "\n",
        "PRED_CODES = list(LANG_CODE_MAP.keys())\n",
        "GT_COLS = list(LANG_CODE_MAP.values())\n",
        "\n",
        "print('Prediction codes:', PRED_CODES)\n",
        "print('Ground-truth columns:', GT_COLS)\n",
        "\n",
        "\n",
        "def read_jsonl(path):\n",
        "    data = []\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    with p.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except Exception as e:\n",
        "                print(f'Warning: skipping invalid JSON line: {e} -- line starts with: {line[:80]}')\n",
        "    return data\n",
        "\n",
        "print('Loading prediction file...')\n",
        "pred_list = read_jsonl(PREDICTIONS_PATH)\n",
        "print(f'Loaded {len(pred_list)} prediction rows')\n",
        "\n",
        "print('Loading ground truth file...')\n",
        "gt_list = read_jsonl(GROUND_TRUTHS_PATH)\n",
        "print(f'Loaded {len(gt_list)} ground truth rows')\n",
        "\n",
        "\n",
        "pred_by_id = {item['id']: item for item in pred_list}\n",
        "gt_by_id = {item['id']: item for item in gt_list}\n",
        "\n",
        "\n",
        "common_ids = sorted(set(pred_by_id.keys()) & set(gt_by_id.keys()))\n",
        "print(f'Found {len(common_ids)} common IDs between predictions and ground-truths')\n",
        "\n",
        "if len(common_ids) == 0:\n",
        "    raise RuntimeError('No overlapping ids found. Check your input files.')\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "comet_available = False\n",
        "comet_model = None\n",
        "try:\n",
        "    from comet import load_from_checkpoint\n",
        "    try:\n",
        "        comet_model = load_from_checkpoint(\"wmt22-comet-da\")\n",
        "        comet_available = True\n",
        "        print('Loaded COMET model wmt22-comet-da')\n",
        "    except Exception:\n",
        "        print('COMET package found but could not load default checkpoint. COMET will be skipped unless you load a model manually.')\n",
        "        comet_available = False\n",
        "except Exception:\n",
        "    print('COMET package not available. To enable COMET, install Unbabel comET: pip install git+https://github.com/Unbabel/comet.git')\n",
        "    comet_available = False\n",
        "\n",
        "\n",
        "for pred_code, gt_col in LANG_CODE_MAP.items():\n",
        "    system_outputs = []\n",
        "    references = []\n",
        "    srcs = []\n",
        "\n",
        "    for _id in common_ids:\n",
        "        p = pred_by_id[_id]\n",
        "        g = gt_by_id[_id]\n",
        "\n",
        "        translations = p.get('translations', {}) or {}\n",
        "        pred_text = translations.get(pred_code)\n",
        "\n",
        "        ref_text = g.get(gt_col)\n",
        "\n",
        "\n",
        "        if pred_text is None or pred_text == '':\n",
        "            continue\n",
        "        if ref_text is None or ref_text == '':\n",
        "            continue\n",
        "\n",
        "        system_outputs.append(pred_text)\n",
        "        references.append(ref_text)\n",
        "        srcs.append(p.get('english_og') or p.get('eng') or p.get('source'))\n",
        "\n",
        "    n = len(system_outputs)\n",
        "    if n == 0:\n",
        "        print(f'No valid pairs for {pred_code} -> {gt_col}. Skipping.')\n",
        "        results.append({\n",
        "            'pred_code': pred_code,\n",
        "            'gt_col': gt_col,\n",
        "            'n': 0,\n",
        "            'chrF': None,\n",
        "            'chrF++': None,\n",
        "            'BLEU': None,\n",
        "            'COMET': None\n",
        "        })\n",
        "        continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        chrf_metric = CHRF(word_order=0, char_order=6)\n",
        "        chrf_score = chrf_metric.corpus_score(system_outputs, [references]).score\n",
        "    except Exception as e:\n",
        "        print('Error computing chrF:', e)\n",
        "        chrf_score = None\n",
        "\n",
        "    try:\n",
        "        chrfpp_metric = CHRF(word_order=2, char_order=6)\n",
        "        chrfpp_score = chrfpp_metric.corpus_score(system_outputs, [references]).score\n",
        "    except Exception as e:\n",
        "        print('Error computing chrF++:', e)\n",
        "        chrfpp_score = None\n",
        "\n",
        "    try:\n",
        "        bleu = sacrebleu.corpus_bleu(system_outputs, [references]).score\n",
        "    except Exception as e:\n",
        "        print('Error computing BLEU:', e)\n",
        "        bleu = None\n",
        "\n",
        "\n",
        "    comet_score_avg = None\n",
        "    if comet_available and comet_model is not None:\n",
        "        try:\n",
        "            data_for_comet = [{'src': s if s is not None else '', 'mt': mt, 'ref': ref} for s, mt, ref in zip(srcs, system_outputs, references)]\n",
        "            scores = comet_model.predict(data_for_comet, batch_size=16)\n",
        "\n",
        "            comet_score_avg = float(np.mean(scores))\n",
        "        except Exception as e:\n",
        "            print('Error running COMET:', e)\n",
        "            comet_score_avg = None\n",
        "\n",
        "    results.append({\n",
        "        'pred_code': pred_code,\n",
        "        'gt_col': gt_col,\n",
        "        'n': n,\n",
        "        'chrF': chrf_score,\n",
        "        'chrF++': chrfpp_score,\n",
        "        'BLEU': bleu,\n",
        "        'COMET': comet_score_avg\n",
        "    })\n",
        "\n",
        "\n",
        "df_res = pd.DataFrame(results)\n",
        "df_res = df_res.sort_values('pred_code').reset_index(drop=True)\n",
        "df_res.to_csv(OUTPUT_CSV, index=False)\n",
        "print('Saved results to', OUTPUT_CSV)\n",
        "\n",
        "df_res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to use\n",
        "1. Upload both JSONL files to the notebook environment and set the filenames in the `PREDICTIONS_PATH` and `GROUND_TRUTHS_PATH` variables.\n",
        "2. Install missing dependencies if any (uncomment pip install lines and run the cell).\n",
        "3. Run the evaluation cell. Results will be saved to `in22_eval_results.csv` and displayed in the notebook.\n",
        "\n",
        "### Notes on COMET\n",
        "- If you want COMET scores, install the Unbabel COMET package and download/load an appropriate checkpoint. The cell attempts to load `wmt22-comet-da` by default; change to a different checkpoint name if you have a different model.\n",
        "- Loading COMET and running predictions requires a GPU for reasonable speed and enough RAM to hold the model.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
